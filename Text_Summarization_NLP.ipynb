{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Summarization_NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOG4wMsrPPAMcVKWj3CKBIj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YouMellouki/Text_Summarization_NLP-from-Article/blob/main/Text_Summarization_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mEua9-88olo"
      },
      "source": [
        "Now lets some Python code to scrape data from the web. The article we are going to scrape is the Wikipedia article on COVID-19.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHE64VNUFGgT"
      },
      "source": [
        "import bs4 as bs\r\n",
        "import urllib.request\r\n",
        "import re\r\n",
        "\r\n",
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/COVID-19_vaccine')\r\n",
        "article = scraped_data.read()\r\n",
        "\r\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml')\r\n",
        "\r\n",
        "paragraphs = parsed_article.find_all('p')\r\n",
        "\r\n",
        "article_text = \"\"\r\n",
        "\r\n",
        "for p in paragraphs:\r\n",
        "    article_text += p.text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJDGilhS8zY1"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dujsaLqVFR9s"
      },
      "source": [
        "#Preprocessing\r\n",
        "\r\n",
        "# Removing Square Brackets and Extra Spaces\r\n",
        "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\r\n",
        "article_text = re.sub(r'\\s+', ' ', article_text)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlNx8vH0FdUr"
      },
      "source": [
        "# Removing special characters and digits\r\n",
        "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\r\n",
        "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_asKrPXc8_tT"
      },
      "source": [
        "Converting Text to Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdYymAZ6Fhc8",
        "outputId": "e0225584-b068-4757-ddbf-360dd1731932"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "sentence_list = nltk.sent_tokenize(article_text)\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X557sf0r9bzY"
      },
      "source": [
        "Weighted Frequency of Occurrence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4LABzMZFpjN",
        "outputId": "bcb69642-3570-49a3-8795-3cc5a0c859d8"
      },
      "source": [
        "nltk.download('stopwords')\r\n",
        "stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "\r\n",
        "word_frequencies = {}\r\n",
        "for word in nltk.word_tokenize(formatted_article_text):\r\n",
        "    if word not in stopwords:\r\n",
        "        if word not in word_frequencies.keys():\r\n",
        "            word_frequencies[word] = 1\r\n",
        "        else:\r\n",
        "            word_frequencies[word] += 1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ANJ5fdHGYXP"
      },
      "source": [
        "maximum_frequncy = max(word_frequencies.values())\r\n",
        "\r\n",
        "for word in word_frequencies.keys():\r\n",
        "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BCMf6T59xCn"
      },
      "source": [
        "Sentence Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKPrZ6CcHCW3"
      },
      "source": [
        "sentence_scores = {}\r\n",
        "for sent in sentence_list:\r\n",
        "    for word in nltk.word_tokenize(sent.lower()):\r\n",
        "        if word in word_frequencies.keys():\r\n",
        "            if len(sent.split(' ')) < 30:\r\n",
        "                if sent not in sentence_scores.keys():\r\n",
        "                    sentence_scores[sent] = word_frequencies[word]\r\n",
        "                else:\r\n",
        "                    sentence_scores[sent] += word_frequencies[word]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GczlW6VJ96th"
      },
      "source": [
        "Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3XWWn2kHN-o",
        "outputId": "feb48fb0-bb06-4bb5-9b33-2aa9b587b737"
      },
      "source": [
        "import heapq\r\n",
        "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\r\n",
        "\r\n",
        "summary = ' '.join(summary_sentences)\r\n",
        "print(summary)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before 2022, 7–10 billion COVID‑19 vaccine doses may be manufactured worldwide, but the sizable pre-orders by affluent countries – called \"vaccine nationalism\" – threaten vaccine availability for poorer nations. The rapid development and urgency of producing a vaccine for the COVID‑19 pandemic may increase the risks and failure rate of delivering a safe, effective vaccine. To combine financial and manufacturing capabilities for a pandemic adjuvanted vaccine technology, GSK joined with Sanofi in an uncommon partnership of multinational companies to support accelerated vaccine development. In October 2020, the CHMP started 'rolling reviews' of the vaccines known as COVID‑19 Vaccine AstraZeneca (ChAdOx1-SARS-CoV-2) and Pfizer-BioNTech COVID-19 Vaccine (BNT162b2). The FDA relies on a Vaccine Adverse Event Reporting System to monitor safety concerns about a vaccine throughout its use in the American public. In December 2020, Interpol warned that organized crime could infiltrate the vaccine supply chain, steal product through physical means, and data theft, or even offer counterfeit vaccine kits. In April 2020, the WHO issued a statement representing dozens of vaccine scientists around the world, pledging collaboration to speed development of a vaccine against COVID‑19.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsnmVeuR-U05"
      },
      "source": [
        "By younesMellouki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrTMFaR7HX1s"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}